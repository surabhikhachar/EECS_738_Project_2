{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'examiner-date-tokens.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2442459</th>\n",
       "      <td>20130703</td>\n",
       "      <td>hassayampa inn steps up to the plate as presco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305868</th>\n",
       "      <td>20130315</td>\n",
       "      <td>do weight loss supplements trigger eating diso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517376</th>\n",
       "      <td>20100713</td>\n",
       "      <td>lacma director wallis annenberg joins with cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249070</th>\n",
       "      <td>20110527</td>\n",
       "      <td>real housewives of new jersey teresa giudice t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190251</th>\n",
       "      <td>20100317</td>\n",
       "      <td>khloe kardashian angers shia labeouf with unso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                    headline_tokens\n",
       "2442459      20130703  hassayampa inn steps up to the plate as presco...\n",
       "2305868      20130315  do weight loss supplements trigger eating diso...\n",
       "517376       20100713  lacma director wallis annenberg joins with cha...\n",
       "1249070      20110527  real housewives of new jersey teresa giudice t...\n",
       "190251       20100317  khloe kardashian angers shia labeouf with unso..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv('../Data/'+filename)\n",
    "df_text = df_text.sample(100000)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont care about the publication date, so we want to focus on the headline tokens. Our first task is to calculate the size of our transition matrix. We will do this by splitting up each headline by word and finding the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstWords = []\n",
    "wordArray = []\n",
    "headlines = df_text['headline_tokens']\n",
    "for headline in headlines:\n",
    "    firstWords.append(headline.split()[0])\n",
    "    for word in headline.split():\n",
    "        wordArray.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = list(set(wordArray))\n",
    "uniqueWords.sort()\n",
    "uniqueWordCount = len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in all headlines: 870254\n",
      "Number of unique words used: 54846\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in all headlines:\", len(wordArray))\n",
    "print(\"Number of unique words used:\", uniqueWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that we have about 26.8 million separate words and 273 thousand unique words. In our algorithm we will be treating each number in the sentence as a word and hopefully the sentences we generate will include numbers and be coherant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict.fromkeys(uniqueWords)\n",
    "index = 0\n",
    "for word in words:\n",
    "    words[word]=index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionCount = np.zeros((uniqueWordCount+1, uniqueWordCount+1))\n",
    "transition2Count = np.zeros((uniqueWordCount+1, uniqueWordCount+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increase the size of the transition matrix by 1 to account for the null state, or a transition from a word to the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlines:\n",
    "    sentence = headline.split()\n",
    "    for i in range(len(sentence)):\n",
    "        if i < len(sentence) - 1:\n",
    "            transitionCount[words[sentence[i]]][words[sentence[i+1]]] += 1\n",
    "        else:\n",
    "            transitionCount[words[sentence[i]]][uniqueWordCount] += 1\n",
    "\n",
    "        if i < len(sentence) - 2:\n",
    "            transition2Count[words[sentence[i]]][words[sentence[i+2]]] += 1\n",
    "        else:\n",
    "            transition2Count[words[sentence[i]]][uniqueWordCount] += 1\n",
    "transitionCount[uniqueWordCount][uniqueWordCount] = 1\n",
    "transition2Count[uniqueWordCount][uniqueWordCount] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "134.0\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "print(transitionCount[words['happy']][words['new']])\n",
    "print(transitionCount[words['new']][words['year']])\n",
    "print(transition2Count[words['happy']][words['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionNorm = transitionCount\n",
    "transition2Norm = transition2Count\n",
    "for i in range(len(transitionCount)):\n",
    "    transitionNorm[i] /= transitionNorm[i].sum()\n",
    "    transition2Norm[i] /= transition2Norm[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007052186177715092\n"
     ]
    }
   ],
   "source": [
    "print(transitionNorm[words['not']][words['not']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n"
     ]
    }
   ],
   "source": [
    "uniqueWords.append(None)\n",
    "print(np.random.choice(uniqueWords, size=1,p=transitionNorm[words['happy']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easter\n",
      "fun\n",
      "at\n",
      "new\n",
      "vegas\n",
      "magazine\n",
      "anniversary\n",
      "edition\n",
      "at\n",
      "the\n",
      "comic\n",
      "book\n",
      "for\n",
      "part\n",
      "1\n",
      "no\n",
      "one\n",
      "is\n",
      "the\n",
      "final\n",
      "fantasy\n",
      "xiv\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "seed = np.random.choice(firstWords, size=1)[0]\n",
    "print(seed)\n",
    "length = 1\n",
    "previousWord = seed\n",
    "nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[previousWord]])[0]\n",
    "\n",
    "while nextWord is not None:\n",
    "    print(nextWord)\n",
    "    length += 1\n",
    "    nextProbabilitys = transitionNorm[words[nextWord]] * ((transition2Norm[words[previousWord]]) != 0)\n",
    "    nextProbabilitys[-1] = min(transitionNorm[words[nextWord]][-1],transition2Norm[words[previousWord]][-1],.01)\n",
    "    nextProbabilitys /= nextProbabilitys.sum()\n",
    "    previousWord = nextWord\n",
    "    nextWord = np.random.choice(uniqueWords, size=1,p=nextProbabilitys)[0]\n",
    "# print(np.random.choice(uniqueWords, size=1,p=transitionNorm[words['happy']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
