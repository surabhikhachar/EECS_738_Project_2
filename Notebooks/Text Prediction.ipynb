{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'examiner-date-tokens.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1389065</th>\n",
       "      <td>20110821</td>\n",
       "      <td>metro detroits summer art fair guide august 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130072</th>\n",
       "      <td>20100222</td>\n",
       "      <td>ice storms create slippery slopes for lawrencians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347814</th>\n",
       "      <td>20110724</td>\n",
       "      <td>why the debt ceiling debate is crucial to los ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885010</th>\n",
       "      <td>20101201</td>\n",
       "      <td>green craft round up north woods ornaments win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757416</th>\n",
       "      <td>20101008</td>\n",
       "      <td>resume writing workshop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                    headline_tokens\n",
       "1389065      20110821  metro detroits summer art fair guide august 29...\n",
       "130072       20100222  ice storms create slippery slopes for lawrencians\n",
       "1347814      20110724  why the debt ceiling debate is crucial to los ...\n",
       "885010       20101201  green craft round up north woods ornaments win...\n",
       "757416       20101008                            resume writing workshop"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv('../Data/'+filename)\n",
    "df_text = df_text.sample(10000)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont care about the publication date, so we want to focus on the headline tokens. Our first task is to calculate the size of our transition matrix. We will do this by splitting up each headline by word and finding the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstWords = []\n",
    "wordArray = []\n",
    "headlines = df_text['headline_tokens']\n",
    "for headline in headlines:\n",
    "    firstWords.append(headline.split()[0])\n",
    "    for word in headline.split():\n",
    "        wordArray.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = list(set(wordArray))\n",
    "uniqueWords.sort()\n",
    "uniqueWordCount = len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in all headlines: 87320\n",
      "Number of unique words used: 16702\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in all headlines:\", len(wordArray))\n",
    "print(\"Number of unique words used:\", uniqueWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that we have about 26.8 million separate words and 273 thousand unique words. In our algorithm we will be treating each number in the sentence as a word and hopefully the sentences we generate will include numbers and be coherant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict.fromkeys(uniqueWords)\n",
    "index = 0\n",
    "for word in words:\n",
    "    words[word]=index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionCount = np.zeros((uniqueWordCount+1, uniqueWordCount+1))\n",
    "transition2Count = np.zeros((uniqueWordCount+1, uniqueWordCount+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increase the size of the transition matrix by 1 to account for the null state, or a transition from a word to the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlines:\n",
    "    sentence = headline.split()\n",
    "    for i in range(len(sentence)):\n",
    "        if i < len(sentence) - 1:\n",
    "            transitionCount[words[sentence[i]]][words[sentence[i+1]]] += 1\n",
    "        else:\n",
    "            transitionCount[words[sentence[i]]][uniqueWordCount] += 1\n",
    "\n",
    "        if i < len(sentence) - 2:\n",
    "            transition2Count[words[sentence[i]]][words[sentence[i+2]]] += 1\n",
    "        else:\n",
    "            transition2Count[words[sentence[i]]][uniqueWordCount] += 1\n",
    "transitionCount[uniqueWordCount][uniqueWordCount] = 1\n",
    "transition2Count[uniqueWordCount][uniqueWordCount] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "14.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(transitionCount[words['happy']][words['new']])\n",
    "print(transitionCount[words['new']][words['year']])\n",
    "print(transition2Count[words['happy']][words['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionNorm = transitionCount\n",
    "transition2Norm = transition2Count\n",
    "for i in range(len(transitionCount)):\n",
    "    transitionNorm[i] /= transitionNorm[i].sum()\n",
    "    transition2Norm[i] /= transition2Norm[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(transitionNorm[words['not']][words['not']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hours']\n"
     ]
    }
   ],
   "source": [
    "uniqueWords.append(None)\n",
    "print(np.random.choice(uniqueWords, size=1,p=transitionNorm[words['happy']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateSentence(seed=np.random.choice(firstWords, size=1)[0], targetLength=7, generatedSentence=[]):\n",
    "    generatedSentence.append(seed)\n",
    "    \n",
    "    nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "    if targetLength > 1:\n",
    "        while(nextWord is None):\n",
    "            nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "\n",
    "    while nextWord is not None:\n",
    "        generatedSentence.append(nextWord)\n",
    "        nextProbabilitys = transitionNorm[words[generatedSentence[-1]]] * (transition2Norm[words[generatedSentence[-2]]]) + transitionNorm[words[generatedSentence[-1]]]/4\n",
    "        nextProbabilitys[-1] += 0.00001\n",
    "        nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) < targetLength - 1:\n",
    "            if nextProbabilitys.sum() > nextProbabilitys[-1]:\n",
    "                nextProbabilitys[-1] /= 10\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) > targetLength + 1:\n",
    "            nextProbabilitys[-1] *= 2\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        nextWord = np.random.choice(uniqueWords, size=1,p=nextProbabilitys)[0]\n",
    "        \n",
    "    return generatedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super bowl next home run fun the houses\n"
     ]
    }
   ],
   "source": [
    "generatedSentence = generateSentence()\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence generator is currently implemented as a Marcov Model as opposed to a Hidden Markov Model. To change this to a HMM, we are going to introduce hidden states that the transition probability matrix is based off of. In many use-cases this hiddnen state may be based off of part of speach of the word. Because with this dataset we are not given the part of speach of this word, we are going to implement a hidden state as a toy model for this assignment. Our hidden state is going to give preference to words based off of their length. The average english word length is 4.5 letters. We are going to have 2 hidden states, which we will refer to as S and L. If our hidden state is S, standing for shorter, we will double probability of all words 4 letters and under, and half the probability of all words longer than 4 letters; vice-versa for stat L, longer word. This isn't exactly ideal in generating coherant sentences as parts of speach may be better, but this will demostrate the Hidden Markov Model and help our model adapt in the future to people's word length preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionProbability(probability, state):\n",
    "    for i in range(len(probability)-1):\n",
    "        if state is 'S':\n",
    "            if len(uniqueWords[i]) < 5:\n",
    "                probability[i] *= 2\n",
    "            else:\n",
    "                probability[i] /= 2\n",
    "        else:\n",
    "            if len(uniqueWords[i]) > 4:\n",
    "                probability[i] *= 2\n",
    "            else:\n",
    "                probability[i] /= 2\n",
    "    probability[i] /= probability.sum()\n",
    "    \n",
    "    \n",
    "hiddenStateDict = {'S': 0, 'L': 1}\n",
    "hiddenStates = ['S','L']\n",
    "hiddenStateTransitionMatrix = [[.7, .3],[.8, .2]]\n",
    "def nextHiddenState(hiddenState):\n",
    "    nextHiddenState = np.random.choice(hiddenStates, size=1, p=hiddenStateTransitionMatrix[hiddenStateDict[hiddenState]])\n",
    "    return nextHiddenState[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceHidden(seed=np.random.choice(firstWords, size=1)[0], targetLength=7, generatedSentence=[], hiddenState = 'S'):\n",
    "    if generatedSentence == []:\n",
    "        generatedSentence.append(seed)   \n",
    "    nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "    if targetLength > 1:\n",
    "        while(nextWord is None):\n",
    "            nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "\n",
    "    while nextWord is not None:\n",
    "        generatedSentence.append(nextWord)\n",
    "        nextProbabilitys = transitionNorm[words[generatedSentence[-1]]] * (transition2Norm[words[generatedSentence[-2]]]) + transitionNorm[words[generatedSentence[-1]]]/4\n",
    "        nextProbability = emissionProbability(nextProbabilitys, hiddenState)\n",
    "        nextProbabilitys[-1] += 0.00001\n",
    "        nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) < targetLength - 1:\n",
    "            if nextProbabilitys.sum() > nextProbabilitys[-1]:\n",
    "                nextProbabilitys[-1] /= 10\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) > targetLength + 1:\n",
    "            nextProbabilitys[-1] *= 2\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        nextWord = np.random.choice(uniqueWords, size=1,p=nextProbabilitys)[0]\n",
    "        \n",
    "        hiddenState = nextHiddenState(hiddenState)\n",
    "    return generatedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweeter accused of time foods market wraps up\n"
     ]
    }
   ],
   "source": [
    "generatedSentence = generateSentenceHidden()\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will demonstrate how to generate a sentence based given a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to find the modern day cookie\n"
     ]
    }
   ],
   "source": [
    "sentence = \"To be or not to\"\n",
    "generatedSentence = generateSentenceHidden(seed=sentence.split()[-1], generatedSentence=sentence.split())\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
