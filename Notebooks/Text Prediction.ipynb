{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'examiner-date-tokens.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2140186</th>\n",
       "      <td>20121124</td>\n",
       "      <td>minecraft reality merges gaming universe with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656214</th>\n",
       "      <td>20100831</td>\n",
       "      <td>great recipes for your late summer vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310901</th>\n",
       "      <td>20110630</td>\n",
       "      <td>whats on wliw tonight nature and nova 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181216</th>\n",
       "      <td>20100314</td>\n",
       "      <td>a clarinet trio experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906534</th>\n",
       "      <td>20141208</td>\n",
       "      <td>bo jackson set to appear madden 15 ultimate te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                    headline_tokens\n",
       "2140186      20121124  minecraft reality merges gaming universe with ...\n",
       "656214       20100831      great recipes for your late summer vegetables\n",
       "1310901      20110630            whats on wliw tonight nature and nova 1\n",
       "181216       20100314                         a clarinet trio experience\n",
       "2906534      20141208  bo jackson set to appear madden 15 ultimate te..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv('../Data/'+filename)\n",
    "df_text = df_text.sample(100000)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont care about the publication date, so we want to focus on the headline tokens. Our first task is to calculate the size of our transition matrix. We will do this by splitting up each headline by word and finding the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstWords = []\n",
    "wordArray = []\n",
    "headlines = df_text['headline_tokens']\n",
    "for headline in headlines:\n",
    "    firstWords.append(headline.split()[0])\n",
    "    for word in headline.split():\n",
    "        wordArray.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = list(set(wordArray))\n",
    "uniqueWords.sort()\n",
    "uniqueWordCount = len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in all headlines: 870907\n",
      "Number of unique words used: 54730\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in all headlines:\", len(wordArray))\n",
    "print(\"Number of unique words used:\", uniqueWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that we have over 800 thousand separate words and 54 thousand unique words. In our algorithm we will be treating each number in the sentence as a word and hopefully the sentences we generate will include numbers and be coherant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict.fromkeys(uniqueWords)\n",
    "index = 0\n",
    "for word in words:\n",
    "    words[word]=index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionCount = np.zeros((uniqueWordCount+1, uniqueWordCount+1))\n",
    "transition2Count = np.zeros((uniqueWordCount+1, uniqueWordCount+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increase the size of the transition matrix by 1 to account for the null state, or a transition from a word to the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlines:\n",
    "    sentence = headline.split()\n",
    "    for i in range(len(sentence)):\n",
    "        if i < len(sentence) - 1:\n",
    "            transitionCount[words[sentence[i]]][words[sentence[i+1]]] += 1\n",
    "        else:\n",
    "            transitionCount[words[sentence[i]]][uniqueWordCount] += 1\n",
    "\n",
    "        if i < len(sentence) - 2:\n",
    "            transition2Count[words[sentence[i]]][words[sentence[i+2]]] += 1\n",
    "        else:\n",
    "            transition2Count[words[sentence[i]]][uniqueWordCount] += 1\n",
    "transitionCount[uniqueWordCount][uniqueWordCount] = 1\n",
    "transition2Count[uniqueWordCount][uniqueWordCount] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionNorm = transitionCount\n",
    "transition2Norm = transition2Count\n",
    "for i in range(len(transitionCount)):\n",
    "    transitionNorm[i] /= transitionNorm[i].sum()\n",
    "    transition2Norm[i] /= transition2Norm[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateSentence(seed=np.random.choice(firstWords, size=1)[0], targetLength=7, generatedSentence=[]):\n",
    "    generatedSentence.append(seed)\n",
    "    \n",
    "    nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "    if targetLength > 1:\n",
    "        while(nextWord is None):\n",
    "            nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "\n",
    "    while nextWord is not None:\n",
    "        generatedSentence.append(nextWord)\n",
    "        nextProbabilitys = transitionNorm[words[generatedSentence[-1]]] * (transition2Norm[words[generatedSentence[-2]]]) + transitionNorm[words[generatedSentence[-1]]]/4\n",
    "        nextProbabilitys[-1] += 0.00001\n",
    "        nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) < targetLength - 1:\n",
    "            if nextProbabilitys.sum() > nextProbabilitys[-1]:\n",
    "                nextProbabilitys[-1] /= 10\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) > targetLength + 1:\n",
    "            nextProbabilitys[-1] *= 2\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        nextWord = np.random.choice(uniqueWords, size=1,p=nextProbabilitys)[0]\n",
    "        \n",
    "    return generatedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple ipad ipod touch of silicon valley film festival\n"
     ]
    }
   ],
   "source": [
    "generatedSentence = generateSentence()\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence generator is currently implemented as a Marcov Model as opposed to a Hidden Markov Model. To change this to a HMM, we are going to introduce hidden states that the transition probability matrix is based off of. In many use-cases this hiddnen state may be based off of part of speach of the word. Because with this dataset we are not given the part of speach of this word, we are going to implement a hidden state as a toy model for this assignment. Our hidden state is going to give preference to words based off of their length. The average english word length is 4.5 letters. We are going to have 2 hidden states, which we will refer to as S and L. If our hidden state is S, standing for shorter, we will double probability of all words 4 letters and under, and half the probability of all words longer than 4 letters; vice-versa for state L, longer word. This isn't exactly ideal in generating coherant sentences as parts of speach may be better, but this will demostrate the Hidden Markov Model and help our model adapt in the future to people's word length preferences. The driving idea is that people don't tend to use many long words in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionProbability(probability, state):\n",
    "    for i in range(len(probability)-1):\n",
    "        if state is 'S':\n",
    "            if len(uniqueWords[i]) < 5:\n",
    "                probability[i] *= 2\n",
    "            else:\n",
    "                probability[i] /= 2\n",
    "        else:\n",
    "            if len(uniqueWords[i]) > 4:\n",
    "                probability[i] *= 2\n",
    "            else:\n",
    "                probability[i] /= 2\n",
    "    probability[i] /= probability.sum()\n",
    "    \n",
    "    \n",
    "hiddenStateDict = {'S': 0, 'L': 1}\n",
    "hiddenStates = ['S','L']\n",
    "hiddenStateTransitionMatrix = [[.7, .3],[.8, .2]]\n",
    "def nextHiddenState(hiddenState):\n",
    "    nextHiddenState = np.random.choice(hiddenStates, size=1, p=hiddenStateTransitionMatrix[hiddenStateDict[hiddenState]])\n",
    "    return nextHiddenState[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little bird flu outbreak survival after responding to increasing clouds of inspiration\n"
     ]
    }
   ],
   "source": [
    "def generateSentenceHidden(seed=None, targetLength=7, generatedSentence=[], hiddenState = 'S'):\n",
    "    if seed is None:\n",
    "        seed=np.random.choice(firstWords, size=1)[0]\n",
    "    if generatedSentence == []:\n",
    "        generatedSentence.append(seed)   \n",
    "    nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "    if targetLength > 1:\n",
    "        while(nextWord is None):\n",
    "            nextWord = np.random.choice(uniqueWords, size=1,p=transitionNorm[words[generatedSentence[-1]]])[0]\n",
    "\n",
    "    while nextWord is not None:\n",
    "        generatedSentence.append(nextWord)\n",
    "        nextProbabilitys = transitionNorm[words[generatedSentence[-1]]] * (transition2Norm[words[generatedSentence[-2]]]) + transitionNorm[words[generatedSentence[-1]]]/4\n",
    "        nextProbability = emissionProbability(nextProbabilitys, hiddenState)\n",
    "        nextProbabilitys[-1] += 0.00001\n",
    "        nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) < targetLength - 1:\n",
    "            if nextProbabilitys.sum() > nextProbabilitys[-1]:\n",
    "                nextProbabilitys[-1] /= 10\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        if len(generatedSentence) > targetLength + 1:\n",
    "            nextProbabilitys[-1] *= 2\n",
    "            nextProbabilitys /= nextProbabilitys.sum()\n",
    "        nextWord = np.random.choice(uniqueWords, size=1,p=nextProbabilitys)[0]\n",
    "        \n",
    "        hiddenState = nextHiddenState(hiddenState)\n",
    "    return generatedSentence\n",
    "generatedSentence = generateSentenceHidden()\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will demonstrate how to generate a sentence based given a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kansas bans this weekend of science poplife\n"
     ]
    }
   ],
   "source": [
    "sentence = \"kansas bans this \"\n",
    "generatedSentence = generateSentenceHidden(seed=sentence.split()[-1], generatedSentence=sentence.split())\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are pretty good considering our methodology and data. It has a similar feel \nand structure of a news headline which was the objective. Overall we are very happy with our results using HMM for text prediction and sentence generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
